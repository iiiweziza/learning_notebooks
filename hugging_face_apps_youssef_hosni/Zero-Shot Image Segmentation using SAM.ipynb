{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2165d1b-eeae-4405-8f69-d082d9ce3bc0",
   "metadata": {},
   "source": [
    "Zero-shot image segmentation, the ability to segment objects in an image without prior training on those specific objects, has become an exciting new frontier in computer vision. The recently released Segment Anything Model (SAM) from Anthropic has emerged as a powerful tool for tackling this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a7879-6ace-438c-8c54-86923e4048bc",
   "metadata": {},
   "source": [
    "# Introduction to Image Segmentation\n",
    "\n",
    "Image segmentation is a process in digital image processing and computer vision that involves dividing an image into multiple segments, regions, or objects. It is used to simplify and change the representation of an image to make it easier to analyze and extract features from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc0427-9df6-4c89-b5e5-0b05e54088f6",
   "metadata": {},
   "source": [
    "# Setting up the Working Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbdfb0a-3fc3-4fae-a686-73f78fb18695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: timm in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (1.0.13)\n",
      "Requirement already satisfied: torch in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from timm) (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from timm) (0.20.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from timm) (0.27.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface_hub->timm) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torchvision->timm) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "Requirement already satisfied: torchvision in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch==2.5.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch==2.5.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\makaa\\anaconda3\\envs\\data_science\\lib\\site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \n",
    "!pip install timm \n",
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d72885-b236-487c-8c2e-9615ad49c9e8",
   "metadata": {},
   "source": [
    "Next, we will define helper functions that we will use throughout this article to plot the boxes on the segmented parts of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2e591c-8ff0-4bdd-8463-efa030bff36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c1a0ac-914e-48db-aef5-d92de5a1c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes a mask overlay on an image.\n",
    "# The Matplotlib axis to plot the mask.\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3),\n",
    "                                np.array([0.6])],\n",
    "                               axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "# Draws a bounding box on an image.\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0),\n",
    "                               w,\n",
    "                               h, edgecolor='green',\n",
    "                               facecolor=(0,0,0,0),\n",
    "                               lw=2))\n",
    "#Displays an image with multiple bounding boxes overlaid.\n",
    "def show_boxes_on_image(raw_image, boxes):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    for box in boxes:\n",
    "      show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "# Displays an image with points overlaid, categorized by labels (positive or negative points).\n",
    "def show_points_on_image(raw_image, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "      labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "      labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "#Combines points and bounding boxes visualization on an image\n",
    "def show_points_and_boxes_on_image(raw_image,\n",
    "                                   boxes,\n",
    "                                   input_points,\n",
    "                                   input_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "      labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "      labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    for box in boxes:\n",
    "      show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "# Combines points and bounding boxes visualization on an image\n",
    "def show_points_and_boxes_on_image(raw_image,\n",
    "                                   boxes,\n",
    "                                   input_points,\n",
    "                                   input_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "      labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "      labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    for box in boxes:\n",
    "      show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "#  Visualizes points with positive and negative labels.\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0],\n",
    "               pos_points[:, 1],\n",
    "               color='green',\n",
    "               marker='*',\n",
    "               s=marker_size,\n",
    "               edgecolor='white',\n",
    "               linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0],\n",
    "               neg_points[:, 1],\n",
    "               color='red',\n",
    "               marker='*',\n",
    "               s=marker_size,\n",
    "               edgecolor='white',\n",
    "               linewidth=1.25)\n",
    "\n",
    "#  Converts a Matplotlib figure into a PIL image.\n",
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img\n",
    "\n",
    "#Displays a mask over an image.\n",
    "def show_mask_on_image(raw_image, mask, return_image=False):\n",
    "    if not isinstance(mask, torch.Tensor):\n",
    "      mask = torch.Tensor(mask)\n",
    "\n",
    "    if len(mask.shape) == 4:\n",
    "      mask = mask.squeeze()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(15, 15))\n",
    "\n",
    "    mask = mask.cpu().detach()\n",
    "    axes.imshow(np.array(raw_image))\n",
    "    show_mask(mask, axes)\n",
    "    axes.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    if return_image:\n",
    "      fig = plt.gcf()\n",
    "      return fig2img(fig)\n",
    "\n",
    "# Visualizes multiple masks generated by a model on an image.\n",
    "def show_pipe_masks_on_image(raw_image, outputs):\n",
    "  plt.imshow(np.array(raw_image))\n",
    "  ax = plt.gca()\n",
    "  for mask in outputs[\"masks\"]:\n",
    "      show_mask(mask, ax=ax, random_color=True)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86641642-3e77-4f52-8200-6cb6081bd511",
   "metadata": {},
   "source": [
    "# Mask Generation with SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44607073-0ef0-4e0d-b9f2-881ec68d5fc0",
   "metadata": {},
   "source": [
    "**The Segment Anything Model (SAM)** is an image segmentation model developed by Meta AI. It can identify the precise location of either specific objects or every object in an image. SAM was released in April 2023 and is open source under the Apache 2.0 license\n",
    "https://segment-anything.com/\n",
    "\n",
    "SAM produces high-quality object masks from input prompts such as points or boxes and can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks and demonstrates strong zero-shot performance on a variety of segmentation tasks.\n",
    "\n",
    "In July 2024, Meta AI released Segment Anything 2 (SAM 2), which is reported to be 6 times more accurate than the original SAM model at image segmentation tasks.\n",
    "\n",
    "The model >= https://huggingface.co/Zigeng/SlimSAM-uniform-77\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0239bbf6-a788-4a78-80ef-985bf2e04515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import pipeline\n",
    "#generator = pipeline(\"mask-generation\", model=\"facebook/sam-vit-huge\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7caf433-3722-4b7a-b17c-8046a253e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f761e3-4590-494f-98f1-443aff27b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = Image.open(r\"F:\\learnings projects\\hugging face with youssef hosni\\zero-shot image classificatio\\dd76db4d5d6bc43560e1d822084dd7cf.png\").convert(\"RGB\")\n",
    "resize_image = raw_image.resize((720, 375))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e65cd2-ba71-4ca2-91c5-baac7d189924",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac717c9e-2aeb-4cc6-9815-74abeeed6f96",
   "metadata": {},
   "source": [
    "The final step is to apply the model to the image and then we will use the show_pipe_masks_on_image to draw the masks on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de78ef0a-3bfa-4449-befb-6e91564200ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = generator(resize_image,points_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c058fc-80d1-4a71-86cb-c09d9a418aa6",
   "metadata": {},
   "source": [
    "points_per_batch: This parameter controls the number of points (pixels or features) that the generator processes in a single batch during inference. It is commonly used to balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726c559-5aa5-4d48-9103-0200fac395a4",
   "metadata": {},
   "source": [
    "#show_pipe_masks_on_image(raw_image, outputs) # put the two togther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f8d38-5f40-4e34-8503-72bd23c3ad9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46aef0b-e18a-42b6-bad1-5e43f2f7ba8e",
   "metadata": {},
   "source": [
    "# Faster Inference: Infer an Image and a Single Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c5fe1f-8f94-430d-bdb7-bb1bf6f2acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForMaskGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Zigeng/SlimSAM-uniform-77\")\n",
    "model = AutoModelForMaskGeneration.from_pretrained(\"Zigeng/SlimSAM-uniform-77\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efbfbd0-1862-49f1-8fd2-e220a57184e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = [[[400, 500]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0b9b3-66ee-41aa-865e-d8ac0538a06c",
   "metadata": {},
   "source": [
    "We will pass the raw image of the points we need to segment and how we want the results to return. We will choose “pt” which refers to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4526eb38-6eae-4e9f-beaa-d672a07252a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(raw_image,input_points = input_points,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2705c53-5cc3-4e68-95e8-26f0fff37c21",
   "metadata": {},
   "source": [
    "Next, we will forward pass through the pre-trained SAM model, using the provided inputs we got from the processor, and retrieve the model's output. The torch.no_grad() context manager is used to disable gradient tracking, as the goal is inference rather than training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318e5bae-61e6-4623-adcb-cce574ec3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00eeac03-7e7c-4025-a582-27531d1f5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's predict the mask for the input image\n",
    "\n",
    "predicted_masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks,\n",
    "    inputs[\"original_sizes\"],\n",
    "    inputs[\"reshaped_input_sizes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9595c760-e4ea-4508-9b1e-0f861835c0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that we got one mask which refers to the object we selected as the input.\n",
    "\n",
    "len(predicted_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "026310a3-1dab-4410-9e2a-d022c22e97ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 900, 1600])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also print the predicted mask shape.\n",
    "\n",
    "predicted_mask = predicted_masks[0]\n",
    "predicted_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2adeab-7056-4d12-97af-981774859325",
   "metadata": {},
   "source": [
    "The pre-trained SAM model produces a single predicted mask with 3 channels and dimensions of 855 x 1300 pixels. Finally, let's print the iou scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54aff71-d014-439a-aa3f-f6d3b6e61d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7597, 0.8831, 0.9271]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007beb5a-ed47-430f-bb93-80bf390df1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Makaa\\AppData\\Local\\Temp\\ipykernel_3804\\2106047152.py:11: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    show_mask_on_image(raw_image, predicted_mask[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841d7bc-6175-4ad3-a14d-7659076f8227",
   "metadata": {},
   "source": [
    "notebook link =>https://www.kaggle.com/code/youssef19/zero-shot-image-segmentation-using-sam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
